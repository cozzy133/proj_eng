{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Determining the Sentiment of Financial News\n",
    "The news database here will train the Naive Bayes\n",
    "For deploying I'd recommend using the NewsAPI code shared and tag the sentiment via the trained NB."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Constructing a Naive Bayes Classifier\n",
    "\n",
    "* Load dataset\n",
    "* Vectorize data\n",
    "* Split data (80/20, train test, random_state=0 so as to allow reproducability)\n",
    "* Initialize the NB classifer and fit\n",
    "* Predict and measure accuracy"
   ]
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "news_pd = pd.read_csv(\"./news_with_sentiment.csv\")\n",
    "news_pd.head()\n",
    "news_pd = news_pd[:2000] # 28,000 rows will use more RAM than is available -> Truncation required\n",
    "\n",
    "cv = CountVectorizer() # Convert text data to a vector as that is required for Naive Bayes\n",
    "X = cv.fit_transform(news_pd['text']).toarray()\n",
    "y = news_pd['sentiment'] # y = the variable we are trying to predict, in this case sentiment"
   ],
   "execution_count": 9,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "# Split train and test data (80/20)\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)\n\n# Initialize the Gaussian Naive Bayes Classifier, then fit the data\nfrom sklearn.naive_bayes import GaussianNB\nclassifier = GaussianNB()\nclassifier.fit(X_train, y_train)",
   "execution_count": 10,
   "outputs": [
    {
     "output_type": "execute_result",
     "execution_count": 10,
     "data": {
      "text/plain": "GaussianNB(priors=None)"
     },
     "metadata": {}
    }
   ]
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "# Predict sentiment of our test data\ny_pred = classifier.predict(X_test)\n\nfrom sklearn.metrics import accuracy_score\nscore = accuracy_score(y_test, y_pred)",
   "execution_count": 11,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "And now we can view the accuracy:"
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "print(score)",
   "execution_count": 12,
   "outputs": [
    {
     "output_type": "stream",
     "text": "0.685\n",
     "name": "stdout"
    }
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Roughly 68% accuracy. Not exactly stellar, if you reduce the dataset further you end up with higher accuracy which is interesting."
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "news_pd = pd.read_csv(\"./news_with_sentiment.csv\")\nnews_pd = news_pd[:1000] # 28,000 rows will use more RAM than is available. Truncation required.\n\ncv = CountVectorizer()\nX = cv.fit_transform(news_pd['text']).toarray()\ny = news_pd['sentiment']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)\n\nclassifier = GaussianNB()\nclassifier.fit(X_train, y_train)\n\ny_pred = classifier.predict(X_test)\n\nscore = accuracy_score(y_test, y_pred)\n\nprint(score)",
   "execution_count": 14,
   "outputs": [
    {
     "output_type": "stream",
     "text": "0.775\n",
     "name": "stdout"
    }
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "77.5% accuracy on a 1000 row dataset with an 80/20 split.\n\nAfter research, Naive Bayes appears to be better with smaller datasets but perhaps we can improve:"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## To improve on our Naive Bayes we can now try a Random Forest:\n\n* Load dataset\n* Remove stopwords, min_df=7 means the data is irrelevant if used in more than 7 documents, max_df of 0.8 means it also is irrelevant if used in more than 80% of documents\n* Vectorize data (max_features is the max number of WORDS in Vector form that will influence the sentiment)\n* Split data (80/20, train test, random_state=0 so as to allow reproducability)\n* Initialize the Random Forest classifer and fit\n* Predict and measure accuracy"
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "# Read in 20,000 headlines\nnews_pd = pd.read_csv(\"./news_with_sentiment.csv\")\nnews_pd = news_pd[:20000] # 28,000 rows will use more RAM than is available. Truncation required.\ny = news_pd['sentiment']",
   "execution_count": 15,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "from nltk.corpus import stopwords\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# Remove stopwords and vectorize the dataset\n#TfidVectorizer converts a collection of raw documents to a matrix of TF-IDF features.\nvectorizer = TfidfVectorizer(max_features=2500, min_df=7, max_df=0.8, stop_words=stopwords.words('english'))\nprocessed_features = vectorizer.fit_transform(news_pd['text']).toarray()",
   "execution_count": 16,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "# 80/20 data split\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(processed_features, y, test_size=0.2, random_state=0)\n\n# Fit our model with split data, starting with 450 estimators (450 decision trees)\nfrom sklearn.ensemble import RandomForestClassifier\n\ntext_classifier = RandomForestClassifier(n_estimators=450, random_state=0)\ntext_classifier.fit(X_train, y_train)",
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "text": "/home/nbuser/anaconda3_420/lib/python3.5/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n  from numpy.core.umath_tests import inner1d\n",
     "name": "stderr"
    }
   ]
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "# Predicting the sentiment of our test data\npredictions = text_classifier.predict(X_test)\n\n\n# Checking our accuracy\nfrom sklearn.metrics import accuracy_score\nprint(accuracy_score(y_test, predictions))",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "93.57% accuracy"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Hyperparameter Tuning:\n\n* Choose a set of trees we want to test\n* Train the model with n trees, store accuracy\n* Loop above until complete\n* Plot the resulting trees v accuracy"
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "from sklearn.ensemble import RandomForestRegressor\n\nestimators = [50, 100, 150, 200, 250, 300, 350, 400, 450, 500, 550, 600, 650, 700, 900, 1000, 1250, 1500, 2000]\naccuracy = []\n\nfor estimator_num in estimators:\n    # Fit and predict\n    text_classifier = RandomForestClassifier(n_estimators=estimator_num, random_state=0)\n    text_classifier.fit(X_train, y_train)\n    predictions = text_classifier.predict(X_test)\n\n    # Store accuracy\n    from sklearn.metrics import accuracy_score\n    accuracy.append(accuracy_score(y_test, predictions))\n\n\n# Graph reported accuracy of various sets of estimators\nimport matplotlib.pyplot as plt\n\nplt.plot(estimators, accuracy)\nplt.ylabel('Accuracy')\nplt.xlabel('Estimators')\nplt.show()\n\nprint(estimators)\nprint(accuracy)",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "![image info](./download.png)\n\nA strange curve? \n\nAs per: https://en.wikipedia.org/wiki/Talk%3ARandom_forest\n\n\"Random Forests does not overfit. The testing performance of Random Forests does not decrease (due to overfitting) as the number of trees increases. Hence after certain number of trees the performance tend to stay in a certain value.\"\n\n\nHowever, we can also see that ~250 estimators/trees is the ideal parameter."
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Naive Bayes v Random Forest v SVM: https://www.researchgate.net/publication/336225950_Comparison_of_Naive_Bayes_Support_Vector_Machine_Decision_Trees_and_Random_Forest_on_Sentiment_Analysis"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Pull Fresh News: "
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "import requests\nimport time\nimport datetime\n\n\narticleCount = 0\n\nheaders = {\n    'User-Agent': 'Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2228.0 Safari/537.36'\n}\n\nstocks = ['TSLA', 'AMZN', 'MMM', 'INTC', 'GOOGL', 'FB', 'MSFT', 'AAPL']\nlist_of_headlines = []\nfor line in stocks:\n    ticker = line\n\n    try:\n\n        #Query for the stock name, for refined news queries.\n        resp = requests.get(\n            url=\"https://www.alphavantage.co/query?function=SYMBOL_SEARCH&keywords={}&apikey=ERO5XRBZNWQ9E608\".format(\n                ticker), headers=headers)\n        data = resp.json()\n        companyName = data['bestMatches'][0]['2. name']\n        print(\"Company Name: \" + companyName)\n\n        #Query for news\n        resp = requests.get(\n            url='https://newsapi.org/v2/everything?'\n'q={}&'\n'from=2020-01-05' # This is the OLDEST date an article can be from, free edition will let you have a month I believe\n'sortBy=popularity&' #Filter by popularity (read the newsapi docs)\n'apiKey=fe00115ceffe418988616191b03e1c74'.format(\n                ticker + \" \" + companyName), headers=headers) #Add the company name in full after the ticker, for more accurate news queries\n        data = resp.json()\n\n        for article in data['articles']:\n            articleCount = articleCount + 1\n            newsTitle = article['title']\n            print(newsTitle)\n            list_of_headlines.append(newsTitle)\n            \n        time.sleep(1)\n\n    except Exception as e:\n        print(\"Error: \" + str(e))\n        time.sleep(10)\n        \n# Create the pandas DataFrame and save to csv\ndf = pd.DataFrame({'headlines':list_of_headlines}) \ndf.to_csv('fresh_news_month_tsla.csv', encoding='utf-8', mode='w', index=False)",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "from nltk.corpus import stopwords\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n# Read in fresh news\nfresh_news = pd.read_csv('./fresh_news_month_tsla.csv')\nfresh_news['headlines'].head(5)\n\n# Vectorize new text data with a max of 40 words being predictors\nvectorizer_new_data = CountVectorizer(max_features=40, min_df=9)\nprocessed_features_new_data = vectorizer_new_data.fit_transform(fresh_news['headlines']).toarray()\n\n# Vectorize training text data with a max of 40 words being predictors\nvectorizer = CountVectorizer(max_features=40, min_df=9)\nprocessed_features = vectorizer.fit_transform(news_pd['text']).toarray()\n\nX_train, X_test, y_train, y_test = train_test_split(processed_features, y, test_size=0.2, random_state=0)\n\n# Predict on new/fresh news after fitting on training data\ntext_classifier = RandomForestClassifier(n_estimators=650, random_state=0)\ntext_classifier.fit(X_train, y_train)\n\npredictions = text_classifier.predict(processed_features_new_data)\n\n# Output our predictions\nprint(predictions)\n\nfor i in range(len(predictions)):\n    if predictions[i] == 1:\n        print(\"Positive: \" + fresh_news['headlines'][i])\n    if predictions[i] == -1:\n        print(\"Negative: \" + fresh_news['headlines'][i])\n        ",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Save our model to disk for production deployment to Sparkbot"
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "from joblib import dump\ndump(text_classifier,'sentimentclassified.joblib')",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "",
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  },
  "language_info": {
   "mimetype": "text/x-python",
   "nbconvert_exporter": "python",
   "name": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4",
   "file_extension": ".py",
   "codemirror_mode": {
    "version": 3,
    "name": "ipython"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}