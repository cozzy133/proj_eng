{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Determining the Sentiment of Financial News using VADER\nThe news database here will train the Naive Bayes (after we label it with VADER)\nFor deploying I'd recommend using the NewsAPI code shared and tag the sentiment via the trained NB."
    },
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true
      },
      "cell_type": "code",
      "source": "import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nnews = pd.read_csv(\"./stockerbot-export1.csv\")\nnews.head(5)",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 1,
          "data": {
            "text/plain": "            id                                               text  \\\n0   1.0197E+18  VIDEO: “I was in my office. I was minding my o...   \n1  1.01971E+18  The price of lumber $LB_F is down 22% since hi...   \n2  1.01971E+18  Who says the American Dream is dead? https://t...   \n3  1.01972E+18  Barry Silbert is extremely optimistic on bitco...   \n4  1.01972E+18  How satellites avoid attacks and space junk wh...   \n\n                        timestamp        source symbols      company_names  \\\n0  Wed Jul 18 21:33:26 +0000 2018  GoldmanSachs      GS  The Goldman Sachs   \n1  Wed Jul 18 22:22:47 +0000 2018    StockTwits       M             Macy's   \n2  Wed Jul 18 22:32:01 +0000 2018     TheStreet     AIG           American   \n3  Wed Jul 18 22:52:52 +0000 2018   MarketWatch     BTC            Bitcoin   \n4  Wed Jul 18 23:00:01 +0000 2018        Forbes    ORCL             Oracle   \n\n                                                 url verified  \n0  https://twitter.com/i/web/status/1019696670777...     TRUE  \n1  https://twitter.com/i/web/status/1019709091038...     TRUE  \n2                            https://buff.ly/2L3kmc4     TRUE  \n3  https://twitter.com/i/web/status/1019716662587...     TRUE  \n4                     http://on.forbes.com/6013DqDDU     TRUE  ",
            "text/html": "<div>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>text</th>\n      <th>timestamp</th>\n      <th>source</th>\n      <th>symbols</th>\n      <th>company_names</th>\n      <th>url</th>\n      <th>verified</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1.0197E+18</td>\n      <td>VIDEO: “I was in my office. I was minding my o...</td>\n      <td>Wed Jul 18 21:33:26 +0000 2018</td>\n      <td>GoldmanSachs</td>\n      <td>GS</td>\n      <td>The Goldman Sachs</td>\n      <td>https://twitter.com/i/web/status/1019696670777...</td>\n      <td>TRUE</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1.01971E+18</td>\n      <td>The price of lumber $LB_F is down 22% since hi...</td>\n      <td>Wed Jul 18 22:22:47 +0000 2018</td>\n      <td>StockTwits</td>\n      <td>M</td>\n      <td>Macy's</td>\n      <td>https://twitter.com/i/web/status/1019709091038...</td>\n      <td>TRUE</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1.01971E+18</td>\n      <td>Who says the American Dream is dead? https://t...</td>\n      <td>Wed Jul 18 22:32:01 +0000 2018</td>\n      <td>TheStreet</td>\n      <td>AIG</td>\n      <td>American</td>\n      <td>https://buff.ly/2L3kmc4</td>\n      <td>TRUE</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1.01972E+18</td>\n      <td>Barry Silbert is extremely optimistic on bitco...</td>\n      <td>Wed Jul 18 22:52:52 +0000 2018</td>\n      <td>MarketWatch</td>\n      <td>BTC</td>\n      <td>Bitcoin</td>\n      <td>https://twitter.com/i/web/status/1019716662587...</td>\n      <td>TRUE</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1.01972E+18</td>\n      <td>How satellites avoid attacks and space junk wh...</td>\n      <td>Wed Jul 18 23:00:01 +0000 2018</td>\n      <td>Forbes</td>\n      <td>ORCL</td>\n      <td>Oracle</td>\n      <td>http://on.forbes.com/6013DqDDU</td>\n      <td>TRUE</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import nltk\n#nltk.download()\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\n\ndef determineSentiment(title):\n    \"\"\"\n    Calculates the weighted sentiment of a piece of text\n    Returns a string which is the determined score of sentiment\n    \"\"\"\n\n    # VADER Polarity Score of Sentiment\n    sia = SIA()\n    results = []\n\n    pol_score = sia.polarity_scores(title)\n    pol_score['news_text'] = title\n    results.append(pol_score)\n\n    ## Check the compound result of the analysis\n    ## Tolerances are near mirrors of those used in academia (though more sensitive to negativity)\n    compound = results[0]['compound']\n    if compound >= 0.05:\n        return 1 # 'Positive'\n    elif compound >= -0.02 and compound < 0.05:\n        return 0 # 'Neutral'\n    elif compound < -0.02:\n        return -1 # 'Negative'",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": "/home/nbuser/anaconda3_420/lib/python3.5/site-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n  warnings.warn(\"The twython library has not been installed. \"\n",
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Let's test out that function and check the sentiment of a piece of negative news:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "test_neg_sentiment = determineSentiment(news.loc[10][\"text\"])\nprint(news.iloc[10][\"text\"]) # Line 10 contains a negative headline (column 'text' is the headline column)\nprint(test_neg_sentiment)",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": "U.S. proposes expedited appeal in fight with AT&amp;T over Time Warner purchase https://t.co/BtjhzdE3Nu https://t.co/Gh32ZJ4vPO\n-1\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# For each text piece, determine the sentiment and append it to a list\n\nsentimentList = []\nfor news_line in news['text']:\n    sentimentList.append(determineSentiment(news_line))",
      "execution_count": 4,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Create a new column, assign the sentiment to each row\nnews['sentiment'] = sentimentList\n\nnews.head(5)",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 5,
          "data": {
            "text/plain": "            id                                               text  \\\n0   1.0197E+18  VIDEO: “I was in my office. I was minding my o...   \n1  1.01971E+18  The price of lumber $LB_F is down 22% since hi...   \n2  1.01971E+18  Who says the American Dream is dead? https://t...   \n3  1.01972E+18  Barry Silbert is extremely optimistic on bitco...   \n4  1.01972E+18  How satellites avoid attacks and space junk wh...   \n\n                        timestamp        source symbols      company_names  \\\n0  Wed Jul 18 21:33:26 +0000 2018  GoldmanSachs      GS  The Goldman Sachs   \n1  Wed Jul 18 22:22:47 +0000 2018    StockTwits       M             Macy's   \n2  Wed Jul 18 22:32:01 +0000 2018     TheStreet     AIG           American   \n3  Wed Jul 18 22:52:52 +0000 2018   MarketWatch     BTC            Bitcoin   \n4  Wed Jul 18 23:00:01 +0000 2018        Forbes    ORCL             Oracle   \n\n                                                 url verified  sentiment  \n0  https://twitter.com/i/web/status/1019696670777...     TRUE          0  \n1  https://twitter.com/i/web/status/1019709091038...     TRUE          0  \n2                            https://buff.ly/2L3kmc4     TRUE         -1  \n3  https://twitter.com/i/web/status/1019716662587...     TRUE          1  \n4                     http://on.forbes.com/6013DqDDU     TRUE         -1  ",
            "text/html": "<div>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>text</th>\n      <th>timestamp</th>\n      <th>source</th>\n      <th>symbols</th>\n      <th>company_names</th>\n      <th>url</th>\n      <th>verified</th>\n      <th>sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1.0197E+18</td>\n      <td>VIDEO: “I was in my office. I was minding my o...</td>\n      <td>Wed Jul 18 21:33:26 +0000 2018</td>\n      <td>GoldmanSachs</td>\n      <td>GS</td>\n      <td>The Goldman Sachs</td>\n      <td>https://twitter.com/i/web/status/1019696670777...</td>\n      <td>TRUE</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1.01971E+18</td>\n      <td>The price of lumber $LB_F is down 22% since hi...</td>\n      <td>Wed Jul 18 22:22:47 +0000 2018</td>\n      <td>StockTwits</td>\n      <td>M</td>\n      <td>Macy's</td>\n      <td>https://twitter.com/i/web/status/1019709091038...</td>\n      <td>TRUE</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1.01971E+18</td>\n      <td>Who says the American Dream is dead? https://t...</td>\n      <td>Wed Jul 18 22:32:01 +0000 2018</td>\n      <td>TheStreet</td>\n      <td>AIG</td>\n      <td>American</td>\n      <td>https://buff.ly/2L3kmc4</td>\n      <td>TRUE</td>\n      <td>-1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1.01972E+18</td>\n      <td>Barry Silbert is extremely optimistic on bitco...</td>\n      <td>Wed Jul 18 22:52:52 +0000 2018</td>\n      <td>MarketWatch</td>\n      <td>BTC</td>\n      <td>Bitcoin</td>\n      <td>https://twitter.com/i/web/status/1019716662587...</td>\n      <td>TRUE</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1.01972E+18</td>\n      <td>How satellites avoid attacks and space junk wh...</td>\n      <td>Wed Jul 18 23:00:01 +0000 2018</td>\n      <td>Forbes</td>\n      <td>ORCL</td>\n      <td>Oracle</td>\n      <td>http://on.forbes.com/6013DqDDU</td>\n      <td>TRUE</td>\n      <td>-1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Saving the results:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "news.to_csv('news_with_sentiment.csv',index=False)\n",
      "execution_count": 6,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Constructing a Naive Bayes Classifier\n\n* Load dataset\n* Vectorize data\n* Split data (80/20, train test, random_state=0 so as to allow reproducability)\n* Initialize the NB classifer and fit\n* Predict and measure accuracy"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from sklearn.feature_extraction.text import CountVectorizer\n\nnews_pd = pd.read_csv(\"./news_with_sentiment.csv\")\nnews_pd = news_pd[:2000] # 28,000 rows will use more RAM than is available -> Truncation required\n\ncv = CountVectorizer() # Convert text data to a vector as that is required for Naive Bayes\nX = cv.fit_transform(news_pd['text']).toarray()\ny = news_pd['sentiment'] # y = the variable we are trying to predict, in this case sentiment",
      "execution_count": 9,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Split train and test data (80/20)\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)\n\n# Initialize the Gaussian Naive Bayes Classifier, then fit the data\nfrom sklearn.naive_bayes import GaussianNB\nclassifier = GaussianNB()\nclassifier.fit(X_train, y_train)",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 10,
          "data": {
            "text/plain": "GaussianNB(priors=None)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Predict sentiment of our test data\ny_pred = classifier.predict(X_test)\n\nfrom sklearn.metrics import accuracy_score\nscore = accuracy_score(y_test, y_pred)",
      "execution_count": 11,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "And now we can view the accuracy:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "print(score)",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": "0.685\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Roughly 68% accuracy. Not exactly stellar, if you reduce the dataset further you end up with higher accuracy which is interesting."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "news_pd = pd.read_csv(\"./news_with_sentiment.csv\")\nnews_pd = news_pd[:1000] # 28,000 rows will use more RAM than is available. Truncation required.\n\ncv = CountVectorizer()\nX = cv.fit_transform(news_pd['text']).toarray()\ny = news_pd['sentiment']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)\n\nclassifier = GaussianNB()\nclassifier.fit(X_train, y_train)\n\ny_pred = classifier.predict(X_test)\n\nscore = accuracy_score(y_test, y_pred)\n\nprint(score)",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": "0.775\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "77.5% accuracy on a 1000 row dataset with an 80/20 split.\n\nAfter research, Naive Bayes appears to be better with smaller datasets but perhaps we can improve:"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## To improve on our Naive Bayes we can now try a Random Forest:\n\n* Load dataset\n* Remove stopwords, min_df=7 means the data is irrelevant if used in more than 7 documents, max_df of 0.8 means it also is irrelevant if used in more than 80% of documents\n* Vectorize data (max_features is the max number of WORDS in Vector form that will influence the sentiment)\n* Split data (80/20, train test, random_state=0 so as to allow reproducability)\n* Initialize the Random Forest classifer and fit\n* Predict and measure accuracy"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Read in 20,000 headlines\nnews_pd = pd.read_csv(\"./news_with_sentiment.csv\")\nnews_pd = news_pd[:20000] # 28,000 rows will use more RAM than is available. Truncation required.\ny = news_pd['sentiment']",
      "execution_count": 15,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from nltk.corpus import stopwords\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# Remove stopwords and vectorize the dataset\n#TfidVectorizer converts a collection of raw documents to a matrix of TF-IDF features.\nvectorizer = TfidfVectorizer(max_features=2500, min_df=7, max_df=0.8, stop_words=stopwords.words('english'))\nprocessed_features = vectorizer.fit_transform(news_pd['text']).toarray()",
      "execution_count": 16,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# 80/20 data split\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(processed_features, y, test_size=0.2, random_state=0)\n\n# Fit our model with split data, starting with 450 estimators (450 decision trees)\nfrom sklearn.ensemble import RandomForestClassifier\n\ntext_classifier = RandomForestClassifier(n_estimators=450, random_state=0)\ntext_classifier.fit(X_train, y_train)",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": "/home/nbuser/anaconda3_420/lib/python3.5/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n  from numpy.core.umath_tests import inner1d\n",
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Predicting the sentiment of our test data\npredictions = text_classifier.predict(X_test)\n\n\n# Checking our accuracy\nfrom sklearn.metrics import accuracy_score\nprint(accuracy_score(y_test, predictions))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "93.57% accuracy"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Hyperparameter Tuning:\n\n* Choose a set of trees we want to test\n* Train the model with n trees, store accuracy\n* Loop above until complete\n* Plot the resulting trees v accuracy"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from sklearn.ensemble import RandomForestRegressor\n\nestimators = [50, 100, 150, 200, 250, 300, 350, 400, 450, 500, 550, 600, 650, 700, 900, 1000, 1250, 1500, 2000]\naccuracy = []\n\nfor estimator_num in estimators:\n    # Fit and predict\n    text_classifier = RandomForestClassifier(n_estimators=estimator_num, random_state=0)\n    text_classifier.fit(X_train, y_train)\n    predictions = text_classifier.predict(X_test)\n\n    # Store accuracy\n    from sklearn.metrics import accuracy_score\n    accuracy.append(accuracy_score(y_test, predictions))\n\n\n# Graph reported accuracy of various sets of estimators\nimport matplotlib.pyplot as plt\n\nplt.plot(estimators, accuracy)\nplt.ylabel('Accuracy')\nplt.xlabel('Estimators')\nplt.show()\n\nprint(estimators)\nprint(accuracy)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "![image info](./download.png)\n\nA strange curve? \n\nAs per: https://en.wikipedia.org/wiki/Talk%3ARandom_forest\n\n\"Random Forests does not overfit. The testing performance of Random Forests does not decrease (due to overfitting) as the number of trees increases. Hence after certain number of trees the performance tend to stay in a certain value.\"\n\n\nHowever, we can also see that ~250 estimators/trees is the ideal parameter."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Naive Bayes v Random Forest v SVM: https://www.researchgate.net/publication/336225950_Comparison_of_Naive_Bayes_Support_Vector_Machine_Decision_Trees_and_Random_Forest_on_Sentiment_Analysis"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Pull Fresh News: "
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import requests\nimport time\nimport datetime\n\n\narticleCount = 0\n\nheaders = {\n    'User-Agent': 'Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2228.0 Safari/537.36'\n}\n\nstocks = ['TSLA', 'AMZN', 'MMM', 'INTC', 'GOOGL', 'FB', 'MSFT', 'AAPL']\nlist_of_headlines = []\nfor line in stocks:\n    ticker = line\n\n    try:\n\n        #Query for the stock name, for refined news queries.\n        resp = requests.get(\n            url=\"https://www.alphavantage.co/query?function=SYMBOL_SEARCH&keywords={}&apikey=ERO5XRBZNWQ9E608\".format(\n                ticker), headers=headers)\n        data = resp.json()\n        companyName = data['bestMatches'][0]['2. name']\n        print(\"Company Name: \" + companyName)\n\n        #Query for news\n        resp = requests.get(\n            url='https://newsapi.org/v2/everything?'\n'q={}&'\n'from=2020-01-05' # This is the OLDEST date an article can be from, free edition will let you have a month I believe\n'sortBy=popularity&' #Filter by popularity (read the newsapi docs)\n'apiKey=fe00115ceffe418988616191b03e1c74'.format(\n                ticker + \" \" + companyName), headers=headers) #Add the company name in full after the ticker, for more accurate news queries\n        data = resp.json()\n\n        for article in data['articles']:\n            articleCount = articleCount + 1\n            newsTitle = article['title']\n            print(newsTitle)\n            list_of_headlines.append(newsTitle)\n            \n        time.sleep(1)\n\n    except Exception as e:\n        print(\"Error: \" + str(e))\n        time.sleep(10)\n        \n# Create the pandas DataFrame and save to csv\ndf = pd.DataFrame({'headlines':list_of_headlines}) \ndf.to_csv('fresh_news_month_tsla.csv', encoding='utf-8', mode='w', index=False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from nltk.corpus import stopwords\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n# Read in fresh news\nfresh_news = pd.read_csv('./fresh_news_month_tsla.csv')\nfresh_news['headlines'].head(5)\n\n# Vectorize new text data with a max of 40 words being predictors\nvectorizer_new_data = CountVectorizer(max_features=40, min_df=9)\nprocessed_features_new_data = vectorizer_new_data.fit_transform(fresh_news['headlines']).toarray()\n\n# Vectorize training text data with a max of 40 words being predictors\nvectorizer = CountVectorizer(max_features=40, min_df=9)\nprocessed_features = vectorizer.fit_transform(news_pd['text']).toarray()\n\nX_train, X_test, y_train, y_test = train_test_split(processed_features, y, test_size=0.2, random_state=0)\n\n# Predict on new/fresh news after fitting on training data\ntext_classifier = RandomForestClassifier(n_estimators=650, random_state=0)\ntext_classifier.fit(X_train, y_train)\n\npredictions = text_classifier.predict(processed_features_new_data)\n\n# Output our predictions\nprint(predictions)\n\nfor i in range(len(predictions)):\n    if predictions[i] == 1:\n        print(\"Positive: \" + fresh_news['headlines'][i])\n    if predictions[i] == -1:\n        print(\"Negative: \" + fresh_news['headlines'][i])\n        ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Save our model to disk for production deployment to Sparkbot"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from joblib import dump\ndump(text_classifier,'sentimentclassified.joblib')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}